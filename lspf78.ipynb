{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COMP2261 Artificial Intelligence \n",
        "==============\n",
        "\n",
        "***Importing the modules and packages needed***\n",
        "\n",
        "\n",
        "The project utilises a range of librarys, most importantly NumPy to allow for the development of the custom implementation model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch \n",
        "import os\n",
        "import mmbra #library for data visualisations\n",
        "import scipy.io as sio\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time #needed for time measurement\n",
        "np.random.seed(42) #for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Create code for custom model implementation***\n",
        "\n",
        "\n",
        "Below is the manual implementation of the Random Forest classifier model. It is built on finding the most popular classification of running multiple recursive decision trees. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Node: #class for the nodes of the decision tree\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        \n",
        "    def is_leaf_node(self): #check if the node is a leaf node\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree: #class for the decision tree\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):   #hyperparameters for the tree\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y): #fit the tree to the data for training\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split): #stop criteria\n",
        "            leaf_value = self._most_common_label(y) \n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh) #split the data between two child nodes\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs): #find the best split for the data based on entropy value\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gained(y, X_column, thr) \n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _information_gained(self, y, X_column, threshold): #calculate the information gain\n",
        "# the IG is the difference between the parent entropy and the weighted avg. of children entropy, attempting reduce it for most information\n",
        "        parent_entropy = self._entropy(y)\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "        \n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y): #helper function to calculate the entropy\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y): #helper function to find the most common label\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X): #predict the labels for the data based on the tree\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "    \n",
        "class RandomForestCustom:\n",
        "    def __init__(self, n_trees=20, max_depth=10, min_samples_split=2, n_feature=None): # hyperparamters that can be tuned\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth=max_depth\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.n_features=n_feature\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y): #fit the random forest to the data for training\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(max_depth=self.max_depth,\n",
        "                            min_samples_split=self.min_samples_split,\n",
        "                            n_features=self.n_features)\n",
        "            X_sample, y_sample = self._bootstrap(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def _bootstrap(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[idxs], y[idxs]\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X): #predict the labels for the data based on the random forest\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
        "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Data Loading***\n",
        "\n",
        "The code first sets up the data directories by constructing paths for different datasets, including brain, image, and text features. It organizes these paths based on the subject identifier, data type (training or testing), and the model used (e.g., image and text models).\n",
        "The datasets are loaded from .mat files using the scipy.io.loadmat() function. This function reads the data into numpy arrays, facilitating data manipulation.\n",
        "\n",
        "***Data Preprocessing***\n",
        "\n",
        "For the brain data, specific time intervals are extracted (70ms-400ms), and the data is reshaped to a two-dimensional format to simplify analysis.\n",
        "Image and text data are scaled to enhance numerical stability during model training.\n",
        "Dimensionality reduction is applied to the image data to limit the number of features, making the dataset more manageable and reducing computational complexity. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwxkPkVI5Qir",
        "outputId": "29ae2edd-0813-4075-954d-86d326b89e63"
      },
      "outputs": [],
      "source": [
        "# declare the global variables\n",
        "brain_seen = 0\n",
        "image_seen = 0\n",
        "text_seen = 0\n",
        "label_seen = 0\n",
        "# creating a function to load the data directories so that this function can be used again when data must be reprocessed and partitioned\n",
        "def load_data():\n",
        "    global brain_seen, image_seen, text_seen, label_seen\n",
        "    data_dir_root = os.path.join('./data', 'ThingsEEG-Text')\n",
        "    sbj = 'sub-10'\n",
        "    image_model = 'pytorch/cornet_s'\n",
        "    text_model = 'CLIPText'\n",
        "    roi = '17channels'\n",
        "    brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
        "    image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
        "    image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
        "    text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
        "\n",
        "    brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
        "    brain_seen = brain_seen[:,:,27:60] # 70ms-400ms\n",
        "    brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
        "    image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double')*50.0\n",
        "    text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double')*2.0\n",
        "    label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
        "    image_seen = image_seen[:,0:100]\n",
        "\n",
        "    brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double')*2.0\n",
        "    brain_unseen = brain_unseen[:, :, 27:60]\n",
        "    brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
        "    image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double')*50.0\n",
        "\n",
        "    brain_seen = torch.from_numpy(brain_seen)\n",
        "    image_seen = torch.from_numpy(image_seen)\n",
        "    text_seen = torch.from_numpy(text_seen)\n",
        "    label_seen = torch.from_numpy(label_seen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_data() #load the data and ensure it has the correct shape and number of features\n",
        "print(brain_seen.shape, image_seen.shape, text_seen.shape, label_seen.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Partition the data***\n",
        "\n",
        "This function is used to divide the data into testing a training then place into a a hstack for training to incorporate all three modalities into the training process. The argument variable n allows the data to be repartitioned for later changes in paradigms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk8PNDL_5U3I",
        "outputId": "504097a4-b856-418f-d957-7cd138449cb9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def partition_data(n):\n",
        "    global brain_seen, image_seen, text_seen, label_seen\n",
        "    index_seen = np.squeeze(np.where(label_seen < 21, True, False)) #get the indices of the labels that are less than 21\n",
        "\n",
        "    brain_seen = brain_seen[index_seen, :]  #get the data of the brain classes that are less than 21\n",
        "    image_seen = image_seen[index_seen, :]  #get the data of the image classes that are less than 21\n",
        "    text_seen = text_seen[index_seen, :] \n",
        "    label_seen = label_seen[index_seen] #get the labels of the classes that are less than 21\n",
        "\n",
        "    num_classes = 20\n",
        "    samples_per_class = 10 \n",
        "\n",
        "    new_train_brain = [] #create a list to store the training data\n",
        "    new_train_image = []\n",
        "    new_train_text = []\n",
        "    new_train_label = []\n",
        "\n",
        "    new_test_brain = []\n",
        "    new_test_image = []\n",
        "    new_test_text = []\n",
        "    new_test_label = []\n",
        "\n",
        "    for i in range(num_classes): #loop through the classes\n",
        "        start_idx = i * samples_per_class#The starting index of the current class\n",
        "        end_idx = start_idx + samples_per_class#The end index of the current class\n",
        "        #Get the data of the current class\n",
        "        class_data_brain = brain_seen[start_idx:end_idx, :]\n",
        "        #Divided into training set and test set\n",
        "        new_train_brain.append(class_data_brain[:n])\n",
        "        new_test_brain.append(class_data_brain[n:])\n",
        "\n",
        "        class_data_image = image_seen[start_idx:end_idx, :]\n",
        "\n",
        "        new_train_image.append(class_data_image[:n])\n",
        "        new_test_image.append(class_data_image[n:])\n",
        "\n",
        "        class_data_text = text_seen[start_idx:end_idx, :]\n",
        "\n",
        "        new_train_text.append(class_data_text[:n])\n",
        "        new_test_text.append(class_data_text[n:])\n",
        "\n",
        "        class_data_label = label_seen[start_idx:end_idx, :]\n",
        "\n",
        "        new_train_label.append(class_data_label[:n])\n",
        "        new_test_label.append(class_data_label[n:])\n",
        "\n",
        "    train_brain = torch.vstack(new_train_brain) #stack the training data into a tensor\n",
        "    train_image = torch.vstack(new_train_image)\n",
        "    train_text = torch.vstack(new_train_text)\n",
        "    train_label = torch.vstack(new_train_label)\n",
        "    test_brain = torch.vstack(new_test_brain)\n",
        "    test_image = torch.vstack(new_test_image)\n",
        "    test_text = torch.vstack(new_test_text)\n",
        "    test_label = torch.vstack(new_test_label)\n",
        "\n",
        "    train_brain_np = train_brain.numpy() #convert the training data to numpy arrays\n",
        "    train_image_np = train_image.numpy()\n",
        "    train_text_np = train_text.numpy()\n",
        "    train_label_np = train_label.numpy().ravel() #convert the training labels to a 1D array\n",
        "\n",
        "    test_brain_np = test_brain.numpy()\n",
        "    test_image_np = test_image.numpy()\n",
        "    test_text_np = test_text.numpy()\n",
        "    test_label_np = test_label.numpy().ravel()\n",
        "\n",
        "    train_features_multiple = np.hstack((train_brain_np, train_image_np, train_text_np)) #stack the training data into a single array\n",
        "    test_features_multiple = np.hstack((test_brain_np, test_image_np, test_text_np)) #stack the test data into a single array\n",
        "    return train_features_multiple, train_label_np, test_features_multiple, test_label_np #return the training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# partition the data into training and test data\n",
        "train_features_multiple, train_label_np, test_features_multiple, test_label_np = partition_data(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Function to run model with implementation***\n",
        "\n",
        "This is the basic custom implementation model, the data still has a high dimensionality hence the training process is very long and yet to be optimised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_model(n_trees = 20, max_depth = 15, min_samples_split = 2):\n",
        "    start = time.time()\n",
        "    model = RandomForestCustom(n_trees, max_depth ,min_samples_split)\n",
        "    model.fit(train_features_multiple, train_label_np)\n",
        "    test_predictions = model.predict(test_features_multiple)\n",
        "    accuracy = accuracy_score(test_label_np, test_predictions)\n",
        "    end = time.time()\n",
        "    time_taken = end-start\n",
        "    print(\"Time taken:\", time_taken)\n",
        "    print(\"Accuracy on test data:\", accuracy)\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(test_label_np, test_predictions))\n",
        "    return accuracy, time_taken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model() #run the model with the default hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Running Baseline model***\n",
        "\n",
        "This is the basic sci-kit learn model, which serves a control for optimising my model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "model.fit(train_features_multiple, train_label_np)\n",
        "test_predictions = model.predict(test_features_multiple)\n",
        "accuracy = accuracy_score(test_label_np, test_predictions)\n",
        "print(\"Accuracy on test data:\", accuracy)\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(test_label_np, test_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Improved model - implementing LDA***\n",
        "\n",
        "Implementing the dimensionality reduction techique to create a greater difference between class features and decreasing convergence speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzAODlyMQisT"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "train_features_multiple = lda.fit_transform(train_features_multiple, train_label_np)\n",
        "test_features_multiple = lda.transform(test_features_multiple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model() #run the model with the default hyperparameters but dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler #scale the data to reduce potential skewness\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_features_multiple_scaled = scaler.fit_transform(train_features_multiple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model() #run the model with the default hyperparameters but scaled data and dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Hyperparameter tuning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding optimal number of trees in the Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "file = open('optimise_n_trees.csv', 'w') #open a file to store the results\n",
        "file.write('n_trees,accuracy,time\\n') \n",
        "max_accuracy = 0\n",
        "optimal_n_trees = 0\n",
        "for i in range(5, 80, 5):\n",
        "    a,t = run_model()\n",
        "    if a > max_accuracy: #store the optimal number of trees and the maximum accuracy\n",
        "        max_accuracy = a\n",
        "        optimal_n_trees = i\n",
        "    file.write(str(i) + ',' + str(a) + ',' + str(t) + '\\n')\n",
        "file.close()\n",
        "print(\"Optimal n_trees:\", optimal_n_trees)\n",
        "print(\"Max accuracy:\", max_accuracy)\n",
        "\n",
        "\n",
        "df = pd.read_csv('optimise_n_trees.csv') #read the file with the results\n",
        "plt.plot(df['n_trees'], df['accuracy']) #plot the accuracy vs the number of trees\n",
        "plt.xlabel('n_trees')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs number of trees') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding optimal max_depth of the tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "file = open('optimise_max_depth.csv', 'w') #open a file to store the results\n",
        "file.write('max_depth,accuracy,time\\n')\n",
        "max_accuracy = 0\n",
        "optimal_max_depth = 0\n",
        "\n",
        "for i in range(1, 20): #loop through the maximum depth of the tree\n",
        "    a,t = run_model()\n",
        "    if a > max_accuracy: #store the optimal maximum depth and the maximum accuracy\n",
        "        max_accuracy = a\n",
        "        optimal_max_depth = i\n",
        "    file.write(str(i) + ',' + str(a) + ',' + str(t) + '\\n')\n",
        "\n",
        "print(\"Optimal max_depth:\", optimal_max_depth) #print the optimal maximum depth and the maximum accuracy\n",
        "print(\"Max accuracy:\", max_accuracy)\n",
        "file.close()\n",
        "\n",
        "df = pd.read_csv('optimise_max_depth.csv') #read the file with the results\n",
        "plt.plot(df['max_depth'], df['accuracy'])\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs maximum tree depth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding optimal minimum sample splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "\n",
        "file = open('optimise_min_samples_split.csv', 'w') #open a file to store the results\n",
        "file.write('min_depth,accuracy,time,\\n')\n",
        "file.write('0,0,0\\n') # setting the minimum samples split to 0 will cause the model to overfit\n",
        "file.write('1,0,0\\n') # setting the minimum samples split to 1 will cause the model to overfit\n",
        "max_accuracy = 0\n",
        "optimal_min_samples_split = 0\n",
        "\n",
        "for i in range(2, 15):\n",
        "    a,t = run_model()\n",
        "    if a > max_accuracy:\n",
        "        max_accuracy = a\n",
        "        optimal_min_samples_split = i\n",
        "    file.write(str(i) + ',' + str(a) + ',' + str(t) + '\\n')\n",
        "\n",
        "print(\"Optimal min_samples_split:\", optimal_min_samples_split) #print the optimal minimum samples split and the maximum accuracy\n",
        "print(\"Max accuracy:\", max_accuracy)   \n",
        "file.close()\n",
        "\n",
        "df = pd.read_csv('optimise_min_samples_split.csv')\n",
        "plt.plot(df.index, df['accuracy'])\n",
        "plt.xticks(df.index)\n",
        "plt.xlabel('min_samples_split')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('accuracy vs min_samples_split')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model with the optimal hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_model(n_trees=optimal_n_trees, max_depth=optimal_max_depth, min_samples_split=optimal_min_samples_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmbra.diverse_evaluation_metrics_example(test_label_np, test_predictions)\n",
        "mmbra.evaluation_visualization_example(test_label_np, test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Introducing new splitting**\n",
        "\n",
        "In this section, variations in the trainig/testing split are used to conclude which is the optimal number of training samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_data() #use the laod data function to load the data again before being split into training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open('different_splits.csv', 'w') #open a file to store the results\n",
        "file.write('no_of_training_values,accuracy,time\\n') #write the header of the file\n",
        "for i in range(2, 10): #loop through the number of training values\n",
        "    train_features_multiple, train_label_np, test_features_multiple, test_label_np = partition_data(i)\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    train_features_multiple = lda.fit_transform(train_features_multiple, train_label_np)\n",
        "    test_features_multiple = lda.transform(test_features_multiple)\n",
        "    a,t = run_model(n_trees=optimal_n_trees, max_depth=optimal_max_depth, min_samples_split=optimal_min_samples_split)\n",
        "    file.write(str(i) + \",\" + str(a) + \",\" + str(t) + '\\n')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('different_splits.csv') #read the file with the results\n",
        "plt.plot(df['no_of_training_values'], df['accuracy']) #plot the accuracy vs the number of training values\n",
        "plt.xlabel('no_of_training_value')\n",
        "plt.ylabel('accuracy') \n",
        "plt.title('accuracy vs train-test split')   \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k-kvTFW_3_FH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
